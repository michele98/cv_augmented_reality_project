{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "from utils.matchers import FeatureMatcher, MultipleInstanceMatcher\n",
    "from utils.visualization import display_frames, display_ar_frames\n",
    "from utils.utils import save_ar_video_f2r, save_ar_video_f2f, save_ar_video\n",
    "\n",
    "#reloads external modules when they are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_filename = './Data/Multiple View.avi'\n",
    "\n",
    "reference_frame = cv2.cvtColor(cv2.imread('Data/ReferenceFrame.png', cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "reference_mask = cv2.imread('Data/ObjectMask.PNG', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "ar_layer = cv2.cvtColor(cv2.imread('Data/AugmentedLayer.PNG', cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)[:,:640]\n",
    "ar_mask = cv2.imread('Data/AugmentedLayerMask.PNG', cv2.IMREAD_GRAYSCALE)[:,:640]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 1000, 500, 100\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax[0].imshow(reference_frame)\n",
    "ax[0].set_title('Reference frame')\n",
    "\n",
    "ax[1].imshow(reference_mask, cmap='gray')\n",
    "ax[1].set_title('Mask')\n",
    "\n",
    "fig.tight_layout(pad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h, dpi = 1000, 500, 100\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax[0].imshow(ar_layer)\n",
    "ax[0].set_title('AR layer')\n",
    "\n",
    "ax[1].imshow(ar_mask, cmap='gray')\n",
    "ax[1].set_title('Mask')\n",
    "\n",
    "fig.tight_layout(pad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frames(video_filename, starting_frame=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch AR layer onto reference frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_frame = reference_frame.copy()\n",
    "ar_frame[ar_mask==255] = ar_layer[ar_mask==255]\n",
    "\n",
    "w, h, dpi = 1200, 400, 100\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(w/dpi, h/dpi), dpi=dpi)\n",
    "\n",
    "ax[0].imshow(reference_frame)\n",
    "ax[0].set_title('Reference frame')\n",
    "\n",
    "ax[1].imshow(ar_layer)\n",
    "ax[1].set_title('AR layer')\n",
    "\n",
    "ax[2].imshow(ar_frame)\n",
    "ax[2].set_title('Combination')\n",
    "\n",
    "fig.tight_layout(pad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do AR on the video\n",
    "Overlay the AR layer onto the video frame.\n",
    "\n",
    "At the beginning, the AR layer and a reference image onto which the AR layer should be stitched are needed.\n",
    "\n",
    "Then the homography between the reference image and the video frame is computed using local invariant features, and it is then applied to the AR layer. The feature algorithms tested here are:\n",
    " - SIFT: accurate, but slow\n",
    " - BRISK: faster, but less accurate\n",
    " - ORB: even faster, even less accurate\n",
    "\n",
    "The homography between the reference image and the video frames is found using local invariant features. The matches between the descriptors are then using FLANN KDTree. The homography on the resulting matches is computed using RANSAC.\n",
    "\n",
    "All this is implemented in the ``matchers.FeatureMatcher`` class.\n",
    "\n",
    "To find the correct homography between the reference image and the video frame 3 approaches are tried:\n",
    " - F2R (frame to reference): here, the homography between the reference image and each video frame is computed. Here, it is harder to find the right correspondance between the reference image and the video frame. The result is jittering of the AR layer, which is made worse by using a low accuracy feature matching algorithm. As can be seen in the result video, SIFT produces the best results here, albeit with a performance hit.\n",
    " - F2F (frame to frame): the homography between the reference image and the first video frame is computed first (in this specific example, it is the identity), and then the homographies between each pair of subsequent frames is computed. This tends to produce smoother results, but the AR layer drifts over time. Less accurate algorithms produce more drift\n",
    " - Hybrid: every 30 frames, a F2F correspondance using SIFT is found, then a F2R with a faster algorithm (BRISK in this case) is done. This has the benefit of both approaches, as it enables to have the performance of the faster algorithm, with limited drift.\n",
    "\n",
    "Having said this, visually the best results are still achieved with F2R using SIFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_filename = './Data/Multiple View.avi'\n",
    "\n",
    "reference_frame = cv2.cvtColor(cv2.imread('Data/ReferenceFrame.png', cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "reference_mask = cv2.imread('Data/ObjectMask.PNG', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "ar_layer = cv2.cvtColor(cv2.imread('Data/AugmentedLayer.PNG', cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)[:,:640]\n",
    "ar_mask = cv2.imread('Data/AugmentedLayerMask.PNG', cv2.IMREAD_GRAYSCALE)[:,:640]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ar_video_f2r(video_filename,\n",
    "                  'out_f2r_sift.avi',\n",
    "                  ar_layer=ar_layer,\n",
    "                  ar_mask=ar_mask,\n",
    "                  reference_image=reference_frame,\n",
    "                  reference_mask=reference_mask,\n",
    "                  algorithm_f2r=cv2.SIFT_create())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BRISK feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ar_video_f2r(video_filename,\n",
    "                  'out_f2r_brisk.avi',\n",
    "                  ar_layer=ar_layer,\n",
    "                  ar_mask=ar_mask,\n",
    "                  reference_image=reference_frame,\n",
    "                  reference_mask=reference_mask,\n",
    "                  algorithm_f2r=cv2.BRISK_create())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORB feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ar_video_f2r(video_filename,\n",
    "                  'out_f2r_orb.avi',\n",
    "                  ar_layer=ar_layer,\n",
    "                  ar_mask=ar_mask,\n",
    "                  reference_image=reference_frame,\n",
    "                  reference_mask=reference_mask,\n",
    "                  algorithm_f2r=cv2.ORB_create())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ar_video_f2f(video_filename,\n",
    "                  'out_f2f_sift.avi',\n",
    "                  ar_layer=ar_layer,\n",
    "                  ar_mask=ar_mask,\n",
    "                  reference_image=reference_frame,\n",
    "                  reference_mask=reference_mask,\n",
    "                  algorithm_f2f=cv2.SIFT_create())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BRISK feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ar_video_f2f(video_filename,\n",
    "                  'out_f2f_brisk.avi',\n",
    "                  ar_layer=ar_layer,\n",
    "                  ar_mask=ar_mask,\n",
    "                  reference_image=reference_frame,\n",
    "                  reference_mask=reference_mask,\n",
    "                  algorithm_f2f=cv2.BRISK_create())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORB feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ar_video_f2f(video_filename,\n",
    "                  'out_f2f_orb.avi',\n",
    "                  ar_layer=ar_layer,\n",
    "                  ar_mask=ar_mask,\n",
    "                  reference_image=reference_frame,\n",
    "                  reference_mask=reference_mask,\n",
    "                  algorithm_f2f=cv2.ORB_create())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid\n",
    "F2F using BRISK with F2R every 30 frames using SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 30\n",
    "save_ar_video(video_filename,\n",
    "              f'out_r{r}.avi',\n",
    "              ar_layer=ar_layer,\n",
    "              ar_mask=ar_mask,\n",
    "              reference_image=reference_frame,\n",
    "              reference_mask=reference_mask,\n",
    "              drift_correction_step=r,\n",
    "              algorithm_f2r=cv2.SIFT_create(),\n",
    "              algorithm_f2f=cv2.BRISK_create())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cv_product_recognition')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b330a1fc137f30f9ef94b968cb21ea387197e87b19802386bf07bfb3ddcfdd68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
